{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d94605b-b89a-4589-9503-cc3d7c99a6e8",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"https://jump.tec.br/wp-content/themes/jump/images/jump.svg\" alt=\"Logomarca da Jump\" style=\"width: 150px; margin-right: 30px;\">\n",
    "  <h1 style=\"font-size: 17px;\">Quinta-Feira 01/08/2024</h1>\n",
    "</div>\n",
    "\n",
    "  <p style=\"color: orange; font-size: 18px; text-align: left; width: 130px; margin-right: 30px;\">Jump Start 4</p>\n",
    "<h2 style=\"text-align: center;\">Thiago Costa Santana</h2>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <h1 style=\"font-size: 22px;\"> Mini Case - PySpark</h1>\n",
    "  <h1 style=\"font-size: 22px;\"> Questionário</h1>\n",
    "  <img src=\"https://quintagroup.com/services/service-images/apache-spark-python-pyspark.jpg\" width=\"250\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c10fa-111d-4f93-a5bc-7b09018b3fd2",
   "metadata": {},
   "source": [
    "***Vocês devem escolher uma questão do case pandas, refazer utilizando PySpark e fazer as seguintes análises:***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3ac02-a8a9-4ec7-adcb-2c957be5d77d",
   "metadata": {},
   "source": [
    "- ### 1- Qual ferramenta foi mais performática?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b807887a-afcf-45c5-ad53-95b6b233031a",
   "metadata": {},
   "source": [
    "R: Nos testes realizados com Pandas e PySpark, constatei que a leitura de código no PySpark é mais eficiente, destacando-se pela sua precisão e capacidade de processamento. Os resultados mais significativos foram observados ao utilizar os parâmetros chunk_size e parquet, com o PySpark demonstrando uma performance superior em termos de tempo de execução e eficiência na leitura do código. Dessa forma, o PySpark se mostrou a ferramenta mais performática nos testes realizados.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576ff63-3f3e-4dd6-b732-38836a23102e",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbdde8de-2f1c-4ad1-b37e-493c2f15a9ce",
   "metadata": {},
   "source": [
    "- ### 2- Existiu diferença de performance entre spark.sql e pyspark puro?\r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0897c4af-d571-4626-adc5-cb0d3e7aecca",
   "metadata": {},
   "source": [
    "R: O desempenho depende muito da estrutura proposta no código, no meu caso no primeiro teste o desempenho superior foi utilizando spark.sql ele possui um otimizador de consultas chamado Catalyst permitindo que de maneira eficaz as consultas SQL sejam otimizadas antes da execução. \r\n",
    "Outro ponto importante seria quanto ao uso em consulta complexas e legibilidade e manutenção do código. \r\n",
    "\r\n",
    "Consultas em SQL tendem a ser mais simples e legíveis para operações complexas de junção, agrupamento e filtragem. Isso pode facilitar o desenvolvimento e redução de erros, além de possibilitar uma execução mais eficiente devido à forma como o otimizador de consultas pode reestruturar as operações.\r\n",
    "\r\n",
    "Já no pyspark no segundo teste tivemos um ganho no tempo de execução, puro podemos também se beneficiar em algumas operações com o uso do otimizador Catalyst porém em operações através de API podem não ser tão bem otimizadas quanto as consulta..sql.\r\n",
    "\r\n",
    "Em minha análise acredito que os testes entre spark.sql e pyspark puro sempre deverão ocorrer, pois em cada estrutura de código proposta pode haver diferenças no tempo de execução.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b5f8d-a1b0-4d7d-86e1-05155c8e2b4a",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363341b-e62a-4117-afc3-b48367dd5909",
   "metadata": {},
   "source": [
    "- ### 3- É possível otimizar os arquivos? Utilizando outro formato, criando particionamento e etc?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005e203-baea-406a-ae75-838d5ca01d8f",
   "metadata": {},
   "source": [
    "R: Sim é possível otimizar os arquivos, em meus testes ficou evidente tanto no pandas utilizando o chunk_size  quanto no pyspark utilizando a transformação de arquivos .tsv para .parquet. Melhorando ainda mais a  performance com o uso de arquivos .parquet evidenciado e comprovado nos testes. \n",
    "\n",
    "Temos mais opções não utilizadas como:\n",
    "\n",
    "\\- ORC: Semelhante ao Parquet é projetado para melhorar a eficiência da leitura e compreensão.\n",
    "\n",
    "\\- Avro: Um formato que é eficiente para serialização de dados. Ele é útil para armazenar dados que mudam com frequência \n",
    "\n",
    "Com particionamento podemos usar outras funções para obter otimização como:\n",
    "\n",
    "\\- coalesce(): Esta função é usada para reduzir o número de partições de um DataFrame. Pode ser útil quando você deseja consolidar dados em menos partições para melhorar a eficiência em operações subsequentes, como gravar arquivos ou executar ações. O coalesce() não envolve um embaralhamento dos dados, o que significa que a operação é mais leve e rápida, mas pode levar a partições desequilibradas.\n",
    "\n",
    "\\- repartition(): Ao contrário de coalesce(), a função repartition() pode aumentar ou diminuir o número de partições e envolve um embaralhamento dos dados. Isso significa que repartition() pode ser usado para melhorar a distribuição dos dados entre as partições, mas pode ser mais custoso em termos de desempenho.\n",
    "\n",
    "\\- partitionBy(): Utilizada geralmente ao escrever DataFrames em um formato de arquivo (como Parquet, JSON, etc.). você está especificando colunas pelas quais os dados devem ser particionados nos arquivos de saída. Isso significa que os dados serão organizados em diretórios separados com base nos valores dessas colunas\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179c8b6-bb51-4097-96fb-59c773c427a1",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6a032b-44f7-4fac-bd50-0c40eb84ed6c",
   "metadata": {},
   "source": [
    "- ### 4. Faça uma pesquisa sobre particionamento no pyspark e como isso pode impactar na performance para análise de BigData?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3a092-e3ae-4636-a3ee-7437ee4286e8",
   "metadata": {},
   "source": [
    "R: O  particionamento é uma técnica fundamental para otimizar o desempenho de operações de leitura e escrita, bem como o processamento paralelo no contexto do PySpark. Essa técnica consiste em dividir um DataFrame ou RDD (Resilient Distributed Dataset) em partes menores, denominadas partições. Cada uma dessas partições é processada de maneira independente por um executor, permitindo assim o processamento paralelo e distribuído de grandes volumes de dados.\n",
    "\n",
    "O particionamento no PySpark tem um impacto significativo na performance da análise de Big Data. Ao entender os princípios básicos e seguir as boas práticas, é possível obter ganhos consideráveis em termos de velocidade e escalabilidade. O particionamento é crucial por diversas razões:\n",
    "\n",
    "\\- Paralelismo: Ao segmentar os dados em partições, o Apache Spark é capaz de executar múltiplas tarefas em paralelo. Isso maximiza a utilização dos recursos disponíveis em um cluster, resultando em um desempenho aprimorado.\n",
    "\n",
    "\\- Eficiência de I/O: Um particionamento eficaz pode reduzir significativamente o tempo requerido para a leitura e escrita de dados. Cada executor é capaz de acessar somente a parte dos dados que lhe é atribuída, o que otimiza o fluxo de entrada e saída.\n",
    "\n",
    "\\- Shuffle:  Operações que demandam redistribuição dos dados, como join, groupByKey e reduceByKey, envolvem um processo denominado shuffle. Um particionamento adequado minimiza o custo deste processo, resultando em um desempenho mais eficiente.\n",
    "\n",
    "Agrupamento: O particionamento pode ser utilizado para agrupar dados que compartilham características semelhantes dentro da mesma partição. Essa estratégia melhora a performance de consultas e análises, pois os dados relevantes são processados juntos.\n",
    "\n",
    "A performance das operações no PySpark pode ser significativamente afetada pelo particionamento, considerando os seguintes aspectos:\n",
    "\n",
    "\\-  Número de Partições:  Um número muito reduzido de partições pode limitar o grau de paralelismo. Por outro lado, um número excessivo de partições pode acarretar um aumento no overhead de gerenciamento. O número ideal de partições depende de variáveis como o volume de dados, a quantidade de núcleos disponíveis no cluster e a natureza das operações a serem realizadas.\n",
    "\n",
    "\\-  Chave de Particionamento:  A escolha da chave de particionamento é essencial para o desempenho em operações como join e groupByKey. Uma boa chave deve garantir uma distribuição uniforme dos dados entre as partições, minimizando o shuffle e otimizando o desempenho.\n",
    "\n",
    "\\-  Skew de Dados:  O desbalanceamento na distribuição dos dados entre as partições, conhecido como skew de dados, pode resultar em um tempo de processamento desigual. Algumas partições podem demandar mais tempo para serem processadas do que outras, impactando negativamente o desempenho geral da aplicação. Utilize técnicas como `coalesce` para reduzir o número de partições excessivas e `repartition` para redistribuir os dados de maneira mais uniforme entre as partições.\n",
    "\n",
    "\\-  Considere o Tipo de Operação:  Diferentes operações podem exigir abordagens distintas para o particionamento. É importante se adaptar às necessidades específicas do que está sendo realizado.\n",
    "\n",
    "\\-  Monitore o Desempenho:  Use ferramentas como o Spark UI para monitorar o desempenho das aplicações e identificar possíveis gargalos, permitindo otimizações contínuas.\n",
    "\n",
    "\n",
    "Exemplos de funcões:\n",
    "\n",
    "\\- coalesce(): Esta função é usada para reduzir o número de partições de um DataFrame. Pode ser útil quando você deseja consolidar dados em menos partições para melhorar a eficiência em operações subsequentes, como gravar arquivos ou executar ações. O coalesce() não envolve um embaralhamento dos dados, o que significa que a operação é mais leve e rápida, mas pode levar a partições desequilibradas.\n",
    "\n",
    "\\- repartition(): Ao contrário de coalesce(), a função repartition() pode aumentar ou diminuir o número de partições e envolve um embaralhamento dos dados. Isso significa que repartition() pode ser usado para melhorar a distribuição dos dados entre as partições, mas pode ser mais custoso em termos de desempenho.\n",
    "\n",
    "\\- partitionBy(): Utilizada geralmente ao escrever DataFrames em um formato de arquivo (como Parquet, JSON, etc.). você está especificando colunas pelas quais os dados devem ser particionados nos arquivos de saída. Isso significa que os dados serão organizados em diretórios separados com base nos valores dessas colunas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0269fd-2e24-4cd9-8f8a-0e88201f6abd",
   "metadata": {},
   "source": [
    "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
